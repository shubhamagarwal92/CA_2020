# Lab material for 9/3/2020

Our lab session today will be divided into three small tasks. The purpose of this lab session is to take a tour and get a flavor of the state-of-the-art (SOTA) methods in NLP and related tasks. 
Have fun and hopefully get inspired by the recent exciting research.

## Task 1: Huggingface transformers demo

This exercise would ideally take 15 minutes. The objective of this demo is to understand how the recent very powerful language models are performing. Understand their limitations and think of ideas as to how we can improve.

- Go to [Write With Transformer](https://transformer.huggingface.co/).
- Select either of the model from 'Checkpoints' or 'Models' section. Start writing.
- Either select "try one of the examples" in the doc or enter your own text.
- Use "tab" or "Trigger Autocomplete" in the menu above.
- Compare the generation capabilities of different models.


## Task 2: Multimodal dialog - Visual Dialog demo

This exercise would also ideally take 15 minutes. Visual Dialog is one of the largest collection of multi-modal dialog currently available also organized as a [challenge](https://visualdialog.org/challenge/2019). In this task also, we will see how these models are still lacking a proper understanding and how they can be repetitive, talk irrelevant things and how current models are still exploiting biases in the dataset that they are trained on. 

- Go to [demo](http://demo.visualdialog.org/) page. 
- Click "Hierarchical Recurrent Encoder". ("Late Fusion" not working as of Jan, 2020.) 
- Upload a pretty simple image and start the conversation (grounded in image) with the bot.


## Task 3: Word embedding demo

This exercise is introduced to give you a practical flavor of what was covered in the lectures. Follow the steps provided.
We highly recommend to use conda for reproducibility. (If things break with the notebook, we won't be sure otherwise.) 

